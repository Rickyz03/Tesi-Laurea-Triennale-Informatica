\chapter{Progettazione e implementazione}
\label{cap:progettazione-implementazione}

\intro{In questo capitolo, vengono descritte le scelte progettuali e le tecniche implementative utilizzate per realizzare il report delle vendite ed il sistema di raccomandazione. Si inizia con una panoramica del flusso delle attività, seguita da una descrizione delle tecnologie e degli strumenti utilizzati. Successivamente, si approfondiscono i vari componenti del sistema sviluppato.}

\section{Flusso delle attività}

\begin{figure}[!h]
    \centering 
    \includegraphics[width=0.9\columnwidth]{activity/Task di analisi delle vendite.png}
    \caption{Flusso delle attività della task di analisi delle vendite}
    \label{fig:activity-sales-analysis}
\end{figure}

Il flusso delle attività per la task di analisi delle vendite è rappresentato nella figura \ref{fig:activity-sales-analysis}. Le attività in sequenza previste dalla task dal lato dell'utente sono:
\begin{itemize}
    \item \textbf{Compilazione dei campi di input}: l'utente deve compilare i campi di input richiesti, cioè deve selezionare il file \gls{csv} contenente i dati delle vendite, selezionare la lingua del report e la valuta da utilizzare per i prezzi, e inserire l'indirizzo email a cui inviare il report;
    \item \textbf{Possibili output}: nel caso in cui l'utente abbia inserito correttamente i campi di input e nel caso la task non abbia riscontrato errori, l'utente visualizzerà un messaggio di esito positivo ed un invito a controllare la propria email per il report generato. Viene inoltre segnalata la generazione delle matrici di raccomandazione, e viene visualizzato il token che serve copiare per poter usufruire della task di raccomandazione. In caso la task non vada a buon fine, l'utente visualizzerà un messaggio di errore;
    \item \textbf{I file generati}: la task genera un file PDF ed un file HTML contenenti il report delle vendite, e l'utente ha la possibilità di scaricarli dall'apposita schermata del portale Oribea;
    \item \textbf{La ricezione della mail}: l'utente riceve una mail all'indirizzo email inserito in precedenza, il cui body contiene l'HTML del report, e riporta come allegati il report delle vendite in formato PDF ed HTML; inoltre, viene mostrato anche qui il token che serve copiare per poter usufruire della task di raccomandazione.
\end{itemize}

\begin{figure}[!h]
    \centering 
    \includegraphics[width=0.9\columnwidth]{activity/Task di raccomandazione di prodotti e clienti.png}
    \caption{Flusso delle attività della task di raccomandazione di prodotti e clienti}
    \label{fig:activity-recommendation-products-customers}
\end{figure}

Il flusso delle attività per la task di raccomandazione di prodotti e clienti è rappresentato nella figura \ref{fig:activity-recommendation-products-customers}. Le attività in sequenza previste dalla task dal lato dell'utente sono:
\begin{itemize}
    \item \textbf{Compilazione dei campi di input}: l'utente deve compilare i campi di input richiesti, cioè deve inserire il token ricevuto nella mail della task di analisi delle vendite, selezionare il tipo di raccomandazione tra "Raccomandare prodotti per un cliente" e "Raccomandare clienti per un prodotto", inserire il nome dell'elemento a cui raccomandare, inserire il numero di raccomandazioni desiderate, e selezionare la lingua con cui presentare la classifica di raccomandazioni;
    \item \textbf{Possibili output}: nel caso in cui l'utente abbia inserito correttamente i campi di input e nel caso la task non abbia riscontrato errori, l'utente visualizzerà la classifica di raccomandazioni generata. In caso la task non vada a buon fine, l'utente visualizzerà un messaggio di errore.
\end{itemize}

Si segnala brevemente, tuttavia, che gli output delle due task non sono stati alla fine l'esatta emulazione di quanto modellato nei diagrammi di attività, ma sono stati modificati in corso d'opera a causa della difficoltà di integrazione con la piattaforma Oribea. In particolare, il download del report delle vendite in formato PDF ed HTML è in realtà stato implementato come download di un file zip (contenente PDF + HTML), poichè la piattaforma Oribea non consente di scaricare più di un file unico come output di una task. Inoltre, il token per accedere alla task di raccomandazione è stato inserito nel file README presente nel file zip, invece che essere mostrato a video come inizialmente previsto. Per quanto riguarda la task di raccomandazione, altrettanto l'output è rappresentato da un file zip, poichè le raccomandazioni sono state generate sia in formato PDF sia in formato \gls{json}, con quest'ultimo che è stato introdotto per favorire una futura estrazione via web dei dati.


\section{Tecnologie e strumenti}
\label{sec:tecnologie-strumenti}

Di seguito viene data una panoramica delle tecnologie e strumenti utilizzati per lo sviluppo delle due task di analisi delle vendite e raccomandazione di prodotti e clienti. Le tecnologie sono state scelte in base alle esigenze del progetto, alla facilità d'uso e alla compatibilità con le altre componenti del sistema.

\subsection{Pandas}
\gls{pandas} è una libreria Python per l'analisi dei dati, che fornisce strutture dati e funzioni per la manipolazione e l'analisi dei dati. È stata utilizzata per leggere i file CSV contenenti i dati delle vendite e per elaborare i dati in fase di preprocessing.

\subsection{Babel}
\gls{babel} è una libreria Python per la gestione della localizzazione e internazionalizzazione delle applicazioni. È stata utilizzata per gestire le date nel report, in modo da poterle visualizzare nel formato corretto in base alla lingua selezionata dall'utente.

\subsection{Anthropic}
\gls{anthropic} è una libreria che consente di interagire con i modelli di linguaggio di grandi dimensioni (\gls{llm}) sviluppati dall'azienda Anthropic tramite le loro API. In particolare, è stato usato il modello Claude 3.7 Sonnet per riconoscere le colonne del file CSV contenente i dati delle vendite, e per generare il resoconto finale che fa da ultimo passaggio del report delle vendite.

\subsection{Matplotlib}
\gls{matplotlib} è una libreria Python per la creazione di grafici e visualizzazioni dei dati. È stata utilizzata per generare i grafici presenti nel report delle vendite, in modo da rendere i dati più comprensibili e visivamente accattivanti.

\subsection{Pillow}
\gls{pillow} è una libreria Python per la manipolazione delle immagini. È stata utilizzata per generare le immagini dei grafici creati con Matplotlib e per la loro conversione in \gls{base64}, in modo da poterli inserire nel report delle vendite in formato PDF ed HTML.

\subsection{ReportLab}
\gls{reportlab} è una libreria Python per la generazione di documenti PDF, che fornisce una disposizione dinamica del contenuto in pagine. È stata utilizzata per creare il report delle vendite in formato PDF, in modo da renderlo scaricabile dal portale Oribea e da poterlo inviare via email all'utente.

\subsection{Jinja2}
\gls{jinja2} è un motore di template per Python, che consente di generare documenti HTML in modo dinamico. È stata utilizzata per creare il report delle vendite in formato HTML, in modo da renderlo visualizzabile direttamente nel browser e da poterlo anche inviare via email all'utente.

\subsection{Server SMTP interno}
Per l'invio di email, è stato utilizzato un server \gls{smtp} messo a disposizione da Oribea, che consente di inviare mail personalizzate senza avere la necessità di utilizzare un indirizzo mail esistente o di collegarsi ad un servizio esterno a pagamento. 

\subsection{SentenceTransformers}
\gls{sentencetransformers} è una libreria Python per la creazione di modelli di linguaggio basati su trasformatori, che consente di generare rappresentazioni vettoriali di frasi e documenti. È stata utilizzata per calcolare le similarità tra le descrizioni dei prodotti e i nomi dei prodotti, in modo da poter generare le raccomandazioni. In particolare, è stato utilizzato il modello \emph{all-MiniLM-L6-v2} per generare le rappresentazioni vettoriali delle descrizioni dei prodotti e dei nomi dei prodotti, e per calcolare la similarità tra di essi.

\subsection{Scikit-learn}
\gls{scikitlearn} è una libreria Python per il machine learning, che fornisce strumenti per la creazione e l'addestramento di modelli di apprendimento automatico. È stata utilizzata per calcolare la similarità tra le descrizioni dei prodotti e i nomi dei prodotti, in modo da poter generare le raccomandazioni. In particolare, è stato utilizzato il metodo \emph{cosine similarity} per calcolare la similarità tra i vettori delle descrizioni dei prodotti e i vettori dei nomi dei prodotti.

\subsection{Zarr}
\gls{zarr} è una libreria Python per la gestione di array multidimensionali, che consente di memorizzare e gestire grandi quantità di dati in modo efficiente. È stata utilizzata per memorizzare su cloud le matrici di raccomandazione generate dalla prima task, in modo da poterle utilizzare successivamente per le raccomandazioni nella seconda task.

\subsection{Google Cloud Storage}
\gls{googlecloudstorage} è un servizio di archiviazione di oggetti su cloud fornito da Google. È stato utilizzato per memorizzare i file generati dalla task di analisi delle vendite, come il report in formato PDF ed HTML, e le matrici di raccomandazione generate dalla task di raccomandazione di prodotti e clienti. In particolare, sono stati utilizzati i pacchetti \emph{google.cloud.storage} e \emph{gcsfs} per interagire con il servizio: \emph{google.cloud.storage} è stato utilizzato per scrivere i file, mentre \emph{gcsfs} è stato utilizzato per leggerli.

\subsection{Google Cloud Functions}
\gls{googlecloudfunctions} è un servizio di calcolo serverless fornito da Google, che consente di eseguire codice in risposta a eventi. È stato utilizzato per caricarvi il codice delle due task di analisi delle vendite e raccomandazione di prodotti e clienti, in modo da poterle eseguire in modo scalabile e senza doversi preoccupare della gestione dei server. Il portale Oribea è configurato per invocare le funzioni di Google Cloud Functions quando l'utente richiede l'esecuzione delle task.

\subsection{Numpy}
\gls{numpy} è una libreria Python per il calcolo scientifico, che fornisce strutture dati e funzioni per la manipolazione di array multidimensionali. È stata utilizzata per calcolare le metriche di valutazione delle raccomandazioni.

\subsection{Black}
\gls{black} è un formattatore di codice Python che consente di mantenere uno stile di codifica coerente e leggibile. È stato utilizzato per formattare il codice delle due task, in modo da renderlo più leggibile e mantenere uno stile di codifica uniforme.

\subsection{Ruff}
\gls{ruff} è un linter per Python che consente di rilevare errori di sintassi e problemi di stile nel codice. È stato utilizzato per verificare la correttezza del codice delle due task, in modo da garantire che fosse privo di errori e conforme agli standard di codifica.

\subsection{MyPy}
\gls{mypy} è un tipo di controllo per Python che consente di verificare la correttezza dei tipi di dati nel codice. È stato utilizzato per verificare la correttezza dei tipi di dati delle due task, in modo da garantire che il codice fosse privo di errori di tipo e conforme agli standard di codifica.

\subsection{Pytest}
\gls{pytest} è un framework di testing per Python che consente di scrivere e eseguire test automatizzati. È stato utilizzato per testare le due task, in modo da garantire che funzionassero correttamente e fossero prive di errori. I test sono stati scritti in modo da coprire i casi d'uso principali delle due task, e sono stati eseguiti automaticamente durante lo sviluppo per garantire la qualità del codice.


\section{Architettura del sistema}

Prima di cominciare a sviluppare le due task, è stata svolta un'analisi preliminare per definire l'architettura del sistema e le interazioni tra le varie componenti. Siccome le task rappresentano due funzionalità distinte, è stato deciso di svilupparle come due funzioni separate su Google Cloud Functions, e quindi come due diversi progetti il più separati possibile, in modo da poterle gestire indipendentemente e poterle testare separatamente.
Per entrambi i progetti, è stato deciso di utilizzare il linguaggio Python, in modo da poter sfruttare le librerie e gli strumenti già descritti nella sezione \ref{sec:tecnologie-strumenti}.
Inoltre, per entrambi è stato scelto un approccio procedurale a discapito di un approccio orientato agli oggetti, poiché le task sono relativamente semplici e non richiedono una struttura complessa. In aggiunta, è stato deciso di creare dei moduli per organizzare il codice e renderne più facile la lettura e la manutenzione.
I moduli in Python sono rappresentati da cartelle e da file con estensione \emph{.py}, e sono stati utilizzati per raggruppare le funzioni correlate e per separare le responsabilità del codice. In particolare, sono stati creati i moduli descritti di seguito.

Per quanto riguarda la task di analisi delle vendite, sono stati creati i seguenti moduli:
\begin{itemize}
    \item \textbf{preparation}: questo modulo, rappresentato da una cartella, contiene tutti i file necessari per l'inizializzazione del sistema e per l'elaborazione dei dati prima dell'analisi delle vendite. In particolare, contiene i seguenti sottomoduli:
    \begin{itemize}
        \item \textbf{Dependency Injection}: questo modulo, rappresentato da un file \emph{.py}, contiene le funzioni per l'iniezione delle dipendenze necessarie per l'esecuzione della task, come i parametri di configurazione delle \gls{api} verso l'LLM Anthropic e verso il server SMTP interno di Oribea, oltre al \gls{modelloembedding} per la similarità;
        \item \textbf{Preprocessing}: questo modulo, rappresentato da un file \emph{.py}, contiene le funzioni per il preprocessing dei dati delle vendite; in particolare, contiene la funzione che va a chiamare l'LLM Anthropic per riconoscere le colonne del file CSV contenente i dati delle vendite, e le funzioni di standardizzazione dei nomi e di sanificazione dei dati;
        \item \textbf{Language Processing}: questo modulo, rappresentato da un file \emph{.py}, contiene le funzioni per l'elaborazione del linguaggio naturale; in particolare, contiene le funzioni per la formattazione delle date e per la selezione dei testi corretti in base alla lingua selezionata dall'utente.
    \end{itemize}
    \item \textbf{report}: questo modulo, rappresentato da una cartella, contiene tutti i file necessari per la generazione del report delle vendite. In particolare, contiene i seguenti sottomoduli:
    \begin{itemize}
        \item \textbf{calculations\_charts}: questo modulo, rappresentato da un file \emph{.py}, contiene le funzioni per il calcolo delle statistiche e dei trend di vendita e per la generazione dei grafici da inserire nel report delle vendite;
        \item \textbf{generate\_report}: questo modulo, rappresentato da un file \emph{.py}, contiene le funzioni per la generazione del report delle vendite in formato PDF ed HTML, utilizzando le librerie ReportLab e Jinja2. Contiene inoltre le funzioni di supporto per la generazione del report, come la funzione di conversione delle immagini dei grafici in base64, la funzione per chiedere all'LLM Anthropic di generare il resoconto finale del report, e le funzioni per la conversione della risposta ottenuta dall'LLM da Markdown verso flowables (per il PDF) e verso HTML;
        \item \textbf{send\_email}: questo modulo, rappresentato da un file \emph{.py}, contiene la funzione per l'invio dell'email all'utente allegando il report delle vendite in formato PDF ed HTML, che utilizza il server SMTP interno di Oribea, e una funzione di supporto per la creazione del body della mail.
    \end{itemize}
    \item \textbf{matrices}: questo modulo, rappresentato da un file \emph{.py}, contiene le funzioni per la generazione delle matrici di raccomandazione, che vengono salvate in oggetti di tipo Zarr. In particolare, contiene le funzioni per la generazione delle matrici di raccomandazione basate sugli acquisti e sulla similarità;
    \item \textbf{main}: questo modulo, rappresentato da un file \emph{.py}, contiene la funzione principale \emph{main} che viene eseguita quando la task viene invocata, e che coordina l'esecuzione delle altre funzioni dei moduli scritti in precedenza. Inoltre, tramite delle funzioni apposite, effettua l'upload in Google Cloud Storage dei file generati dalla task, come il report delle vendite in formato PDF ed HTML e le matrici di raccomandazione. Infine, il \emph{main} restituisce l'output della task, cioè una cartella compressa contenente il report delle vendite in formato PDF ed HTML, ed un file README che contiene il token per accedere alla task di raccomandazione di prodotti e clienti.
\end{itemize}

Per quanto riguarda la task di raccomandazione di prodotti e clienti, sono stati creati i seguenti moduli:
\begin{itemize}
    \item \textbf{prediction}: questo modulo, rappresentato da una cartella, contiene tutti i file necessari per la predizione delle raccomandazioni. In particolare, contiene i seguenti sottomoduli:
    \begin{itemize}
        \item \textbf{predictor}: questo modulo, rappresentato da un file \emph{.py}, contiene le funzioni dirette per la predizione delle raccomandazioni, che utilizzano le matrici di raccomandazione generate dalla task di analisi delle vendite. Esso contiene anche l'algoritmo di rank fusion, che combina assieme le raccomandazioni ottenute dalle matrici;
        \item \textbf{filter}: questo modulo, rappresentato da un file \emph{.py}, contiene le funzioni per il filtraggio delle raccomandazioni, in modo da rimuovere, a seconda del caso, i prodotti già acquistati dall'utente ricevuto in input oppure gli utenti che hanno già acquistato il prodotto ricevuto in input;
        \item \textbf{make\_prediction}: questo modulo, rappresentato da un file \emph{.py}, contiene le funzioni per la creazione della predizione delle raccomandazioni, che utilizzano le funzioni dei moduli \emph{predictor} e \emph{filter} per generare la classifica di raccomandazioni. Contiene inoltre una funzione di explanation che stampa su console i passaggi intermedi della predizione, per facilitare il debug e la comprensione del funzionamento della task;
        \item \textbf{build\_output}: questo modulo, rappresentato da un file \emph{.py}, costruisce l'output della task, cioè chiama una funzione che genera un file JSON con le raccomandazioni e i punteggi, e una funzione che genera un PDF dove viene riportato il ranking predetto.
    \end{itemize}
    \item \textbf{evaluation}: questo modulo, rappresentato da una cartella, contiene tutti i file necessari per la valutazione delle raccomandazioni. In particolare, contiene i seguenti sottomoduli:
    \begin{itemize}
        \item \textbf{metrics}: questo modulo, rappresentato da un file \emph{.py}, contiene le funzioni per il calcolo delle metriche di valutazione delle raccomandazioni. Queste metriche vengono calcolate confrontando le raccomandazioni generate dalla task con le vendite passate dei prodotti o dei clienti;
        \item \textbf{check\_metrics}: questo modulo, rappresentato da un file \emph{.py}, contiene le funzioni per il controllo delle metriche di valutazione delle raccomandazioni, in modo da verificare se le raccomandazioni generate sono valide e soddisfacenti. In particolare, vengono richiamate le funzioni del modulo \emph{metrics} per calcolare le metriche di valutazione, che vengono stampate su console, assieme al loro significato, per facilitare il debug e la comprensione del funzionamento del sistema di raccomandazione.
    \end{itemize}
    \item \textbf{main}: questo modulo, rappresentato da un file \emph{.py}, contiene la funzione principale \emph{main} che viene eseguita quando la task viene invocata, e che coordina l'esecuzione delle altre funzioni dei moduli scritti in precedenza. Inizialmente, essa si occupa di cercare in Google Cloud Storage il bucket identificato dal token ricevuto in input, e di caricare le matrici di raccomandazione da esso. Successivamente, il \emph{main} richiama le funzioni del modulo \emph{prediction/make\_prediction} per generare la classifica di raccomandazioni, e quelle del modulo \emph{evaluation/check\_metrics} per calcolare le metriche di valutazione di tali raccomandazioni. Infine, il \emph{main} restituisce l'output della task, cioè una cartella compressa contenente le raccomandazioni in formato JSON e PDF.
\end{itemize}

Sono stati appena descritti i moduli contenuti dentro la cartella \emph{src} delle task, che rappresentano il cuore del codice dei due progetti. Attorno ad esse, per ciascuna task, è stata creata una struttura di cartelle e file che rappresentano la configurazione del progetto, le dipendenze necessarie per l'esecuzione delle task, i test automatizzati e un file main per collegarsi al servizio Cloud Functions. In particolare, sono stati creati i seguenti file:
\begin{itemize}
    \item \textbf{requirements.txt}: questo file contiene le dipendenze necessarie per l'esecuzione delle task, che vengono installate automaticamente quando si carica il codice su Google Cloud Functions;
    \item \textbf{.gcloudignore}: questo file contiene le regole per ignorare i file e le cartelle che non devono essere caricati su Google Cloud Functions, come i file di test e i file di configurazione locali;
    \item \textbf{tests}: questa cartella contiene i test automatizzati per le due task, che vengono eseguiti automaticamente durante lo sviluppo per garantire la qualità del codice. I test sono scritti utilizzando il framework Pytest, e sono organizzati in moduli separati per ciascuna task;
    \item \textbf{main.py}: questo file contiene il codice principale per collegarsi al servizio Cloud Functions, e richiama la funzione principale della task corrispondente;
    \item \textbf{logger.py}: questo file contiene la configurazione dell'oggetto logger delle due task, che viene utilizzato per tracciarne l'esecuzione e per facilitarne il debug in caso di errori. I log vengono scritti su console e possono essere visualizzati tramite il servizio di logging di Google Cloud;
    \item \textbf{.env}: questo file contiene le variabili d'ambiente necessarie per l'esecuzione delle task;
    \item \textbf{pyproject.toml}: questo file contiene la configurazione del progetto, come il nome del progetto, la versione, la descrizione e le dipendenze necessarie per l'esecuzione delle task. Inoltre, contiene le configurazioni per i tool di formattazione e linting del codice, come Black, Ruff e MyPy;
    \item \textbf{pytest.ini}: questo file contiene la configurazione per Pytest, il framework di testing utilizzato per i test automatizzati delle due task. In particolare, contiene la configurazione della coverage a 75\% per i test, e la configurazione per eseguire i test per l'LLM separatamente rispetto ai test "classici";
    \item \textbf{file run\_*.bat}: script per l'esecuzione dell'analisi statica del codice, che eseguono i tool di formattazione e linting Black, Ruff e MyPy. Questi script sono stati creati per facilitare l'esecuzione di questi ultimi tool, in modo da poterli consultare facilmente durante lo sviluppo;
    \item \textbf{deploy.bat}: questo file contiene lo script per l'esecuzione del complesso comando di deployment su Google Cloud Functions.
\end{itemize}

Successivamente, verso il termine dello stage, a queste cartelle contenenti i progetti backend delle due task si sono aggiunte le due cartelle dei corrispettivi progetti frontend, la cui realizzazione era desiderabile (non obbligatoria), che sono stati sviluppati per permettere all'utente di interagire con le task tramite un'interfaccia diversa da quella standard della piattaforma Oribea, e per introdurre la validazione dell'input. La struttura di queste cartelle corrisponde alla struttura standard di un progetto React, i cui componenti sono approfonditi nella sezione \S\ref{sec:frontend}.



\section{Preprocessing}
\label{sec:preprocessing}

Il preprocessing dei dati delle vendite è un passaggio fondamentale per garantire che i dati siano pronti per l'analisi e la generazione del report, e anche perchè sia possibile crearci le matrici di raccomandazione.
In questa sezione, vengono descritti i passaggi principali del preprocessing, che sono stati implementati nella task di analisi delle vendite.

Una volta ricevuto in input il file csv con i dati delle vendite, il suo contenuto viene prelevato mediante Pandas e salvato in oggetto \gls{dataframe}. La prima cosa che è stato scelto di fare è una pulizia e sanificazione dei dati, in modo da rimuovere eventuali errori o anomalie. In particolare, vengono rimosse eventuali righe duplicate e vengono tolti gli spazi bianchi all'inizio e alla fine nei valori di tipo stringa.

A questo punto, procedere meccanicamente non è più possibile, poichè ogni file csv di vendite possiede i propri specifici nomi delle colonne, che possono essere scritti in modi diversi, e che non sono standardizzati. Per questo motivo, è stato scelto di utilizzare un modello di LLM per riconoscere le colonne del file CSV, in modo da poterle standardizzare e rendere più facili da gestire. In particolare, viene utilizzato il modello Claude 3.7 Sonnet di Anthropic per riconoscere le colonne del file CSV e restituirne i nomi corrispondenti alle categorie prestabilite, in modo da poterci svolgere la successiva analisi delle vendite e generazione delle matrici di raccomandazione. Il modello viene chiamato tramite l'API di Anthropic, e vengono passati i nomi delle colonne e un'anteprima del loro contenuto come input. Il modello viene istruito con un apposito header a restituire i nomi delle colonne che facciano da parametri ad alcune particolari funzioni, cioè:
\begin{itemize}
    \item \textbf{prepare\_fundamental\_columns}: questa funzione prende in input i nomi delle colonne fondamentali del file CSV, cioè gli id di clienti e prodotti, le date degli ordini, i prezzi e le quantità, modifica tali colonne nel dataframe standardizzandone i nomi, e restituisce il dataframe modificato. Vengono inoltre formattate numericamente le colonne dei prezzi e delle quantità (anche tramite la funzione di supporto \texttt{replace\_and\_to\_numeric}), in modo da poterle utilizzare per i calcoli successivi, e vengono rimosse le righe con almeno un valore vuoto;
    \item \textbf{prepare\_denominative\_columns}: questa funzione prende in input i nomi delle colonne denominative del file CSV, cioè le colonne contenenti le descrizioni dei prodotti e dei clienti, e modifica tali colonne nel dataframe standardizzandone i nomi. È possibile che l'LLM associ più colonne allo scopo descrittivo di un prodotto o di un cliente, e in tal caso queste colonne vengono concatenate in una sola, che contiene così la descrizione completa. Inoltre, vengono rimosse le righe con almeno un valore vuoto, e tutte le colonne, fondamentali e denominative, vengono ordinate tra loro in modo da avere un ordine standardizzato;
    \item \textbf{extract\_date\_components}: questa funzione prende in input il nome della colonna contenente il timestamp degli ordini e un parametro che indica se nel timestamp viene prima il valore del giorno o del mese, e crea quattro nuove colonne nel dataframe, che contengono rispettivamente il giorno, la settimana, il mese e l'anno dell'ordine. Inoltre, viene gestita la possibilità che la stessa colonna di timestamp presenti i suoi valori in due formati diversi, provando un secondo tentativo di parsing della data in caso di errore nel primo tentativo. Se neanche il secondo tentativo va a buon fine, vengono rimosse le righe contenenti i timestamp non validi.
\end{itemize}

Terminata la parte di preprocessing dipendente dal dataset, vengono effettuate alcune ultime elaborazioni meccaniche:
\begin{itemize}
    \item \textbf{create\_iso\_date\_columns}: questa funzione crea due nuove colonne nel dataframe, che contengono rispettivamente la data in formato ISO e il mese in formato ISO, a partire dalle colonne del giorno, del mese e dell'anno create in precedenza;
    \item \textbf{sort\_values}: questa funzione ordina il dataframe in base alla colonna della data ISO degli ordini, in modo da avere le righe ordinate cronologicamente;
    \item \textbf{maintain\_first\_name}: questa funzione prende in input una coppia di colonne contenenti id e nomi di una stessa entità, e fa un modo che ad uno stesso id sia sempre associato un solo nome, prendendo il primo nome trovato per quell'id e mantenendolo per tutte le righe in cui quell'id compare. In questo modo, si evita di avere nomi diversi associati allo stesso id, come era capitato per alcuni prodotti, che potrebbero causare errori durante l'analisi dei dati;
    \item \textbf{delete\_columns\_except}: questa funzione prende in input una lista di nomi di colonne da mantenere nel dataframe, e rimuove tutte le altre colonne. In questo modo, si evita di avere colonne inutili o ridondanti nel dataframe, che potrebbero causare confusione durante l'analisi dei dati.
\end{itemize}

Dopo aver eseguito tutte queste operazioni, il dataframe è pronto per essere utilizzato per l'analisi delle vendite e la generazione delle matrici di raccomandazione. In particolare, il dataframe contiene le colonne fondamentali e denominative standardizzate, le colonne con i componenti della data degli ordini, e le colonne con i prezzi e le quantità formattate numericamente. Inoltre, il dataframe è ordinato cronologicamente in base alla data degli ordini, e contiene solo le colonne necessarie per l'analisi dei dati.

Inizialmente, per migliorare il report delle vendite con ulteriori informazioni sui prodotti, era stato pensato di aggiungere due altre colonne al dataframe, una per la marca e una per la categoria del prodotto. Tuttavia, come descritto nelle sezioni \S\ref{sec:recognition-brands} e \S\ref{sec:recognition-categories}, ciò non è stato possibile.


\subsection{Riconoscimento delle marche}
\label{sec:recognition-brands}

Per poter analizzare le vendite in modo più dettagliato, è stato pensato di introdurre una colonna "Marca" nei dataset, che potesse contenere il nome della marca del prodotto presente in tale riga. Per ottenere ciò, era dunque necessario sviluppare un sistema di riconoscimento delle marche. Sono state provate le seguenti strategie:
\begin{itemize}
    \item \textbf{Riconoscimento tramite dizionario}: è stato creato un dizionario contenente le marche più comuni, ma si è rivelato poco efficace, poiché molte marche non erano presenti nel dizionario e il riconoscimento era limitato;
    \item \textbf{Riconoscimento tramite regex}: è stato provato a utilizzare delle espressioni regolari per riconoscere le marche, ma si è rivelato poco efficace, poiché molte marche non seguivano uno schema comune e il riconoscimento era limitato;
    \item \textbf{Riconoscimento tramite modelli di \gls{ml}{}}: è stato pensato di creare un modello di \gls{ml} per riconoscere le marche, ma si è rivelato poco efficace, poiché il dataset non era sufficientemente grande e vario per addestrare un modello affidabile, e soprattuto non erano disponibili le etichette necessarie per un addestramento supervisionato;
    \item \textbf{Riconoscimento tramite modelli di \gls{ner}{}}: è stato provato ad utilizzare un modello di \gls{ner} sulle descrizioni dei prodotti e a selezionare le entità segnalate di tipo \emph{ORG} (organizzazione), ma si è rivelato poco efficace, poiché il modello non era stato addestrato specificamente per questo compito e il riconoscimento era limitato;
    \item \textbf{Riconoscimento tramite LLM}: è stato provato ad utilizzare un modello di linguaggio di grandi dimensioni (LLM) per riconoscere le marche, e si è rivelato mediamente efficace, ma il riconoscimento di ciascuna marca richiedeva troppo tempo e tantissime chiamate API; allora, è stato fatto un tentativo di raggruppamento di più descrizioni da inviare assieme per il riconoscimento di più marche contemporaneamente, ma si è rivelato poco efficace, poiché il modello ogni tanto si dimenticava di alcune marche o ne aggiungeva qualcuna in più, totalmente senza motivo (avvenivano cioè le cosidette "allucinazioni").
\end{itemize}

Dunque, si è deciso di non implementare il riconoscimento delle marche nel sistema di analisi automatizzato, poiché non era possibile garantire un riconoscimento affidabile e preciso.


\subsection{Riconoscimento delle categorie}
\label{sec:recognition-categories}

Per poter analizzare le vendite in modo più dettagliato, è stato pensato di introdurre una colonna "Categoria" nei dataset, che potesse contenere il nome della categoria del prodotto presente in tale riga. Per ottenere ciò, era dunque necessario sviluppare un sistema di riconoscimento delle categorie.

Escluse le opzioni già descritte nella sezione \S\ref{sec:recognition-brands} per il riconoscimento delle marche, è stato pensato di utilizzare un modello \gls{kmeans}{} per raggruppare i prodotti in base alle loro descrizioni, in modo da ottenere delle categorie. Tuttavia, ciò si è rivelato poco efficace, poiché il modello non era in grado di raggruppare i prodotti in categorie in modo affidabile e preciso, e il numero di categorie era troppo elevato per poterle gestire manualmente.

Dunque, si è deciso di non implementare il riconoscimento delle categorie nel sistema di analisi automatizzato, poiché non era possibile garantire un riconoscimento affidabile e preciso.


È stata svolta anche un'altra operazione di preprocessing, che però non è stata implementata nel modulo dedicato.
Infatti, durante lo sviluppo dei grafici per il report delle vendite, è stato necessario gestire il caso in cui prodotti diversi abbiano lo stesso nome: è stato deciso di inserire un suffisso numerico (es.: (1), (2), ...) ai nomi di prodotto identici, in modo da poterli distinguere. Questa modifica si è però rivelata molto lenta ed inefficiente da svolgere su tutto il dataframe, e quindi è stato deciso di svolgerla solo sui nomi dei prodotti che venivano utilizzati per i grafici.
Questa operazione è stata dunque svolta direttamente nel modulo \emph{report/calculations\_charts}, dove vengono generati i grafici, e non nel modulo \emph{preparation/preprocessing}, dove viene svolto il preprocessing dei dati delle vendite.



\section{Language processing}
\label{sec:language-processing}

In ogni applicazione che gestisce testi in lingue diverse è fondamentale avere un sistema di elaborazione del linguaggio naturale che consenta di gestire correttamente le lingue e le loro specificità. Nel caso della task di analisi delle vendite, è stato necessario implementare un sistema di language processing che consentisse di gestire correttamente le date e i testi in base alla lingua selezionata dall'utente.

Le lingue italiano, inglese, francese, spagnolo e tedesco sono le uniche permesse nel sistema: ne viene selezionata una dall'utente tramite un menù a tendina presente nell'interfaccia grafica della piattaforma Oribea. Esse sono state scelte in quanto sono le lingue più parlate al mondo e le più comuni tra gli utenti del piattaforma.

Per quanto riguarda le date, è stato necessario implementare un sistema di formattazione che consentisse di gestirle correttamente in base alla lingua selezionata dall'utente. In particolare, oltre ad avere le date in formato ISO (le cui relative colonne sono state ottenute nell'elaborazione descritta in \S\ref{sec:preprocessing}), in modo da poterle utilizzare per i calcoli, occorreva anche un sistema che consentisse di gestire le date in formato localizzato, in modo da poterle visualizzare correttamente nel report delle vendite.
Le relative colonne sono state ottenuto utilizzando la libreria \emph{babel} di Python, che consente di gestire le lingue e le loro specificità. In particolare, è stato utilizzato il modulo \emph{babel.dates} e la funzione \emph{format\_dates} per formattare le date in base alla lingua selezionata dall'utente.

Per quanto riguarda i testi presenti nel report delle vendite, è stato necessario implementare un sistema di selezione dei testi in base alla lingua selezionata dall'utente. Considerata la relativa semplicità del report, è stato deciso di non utilizzare dei file di lingua, bensì sono stati semplicemente scritti i testi selezionabili in modo \gls{hardcoded} direttamente nel codice.
La funzione dedicata ha dunque il compito di restituire un dizionario che presenta i testi nella lingua corretta accessibili tramite le chiavi corrispondenti. Ad esempio, per inserire il titolo del report, si deve accedere alla chiave \emph{title} del dizionario \texttt{report\_language\_dependent\_data}, che permette di ottenere il titolo nella lingua selezionata dall'utente.

Un'altra operazione di language processing riguarda la selezione del testo che la task deve riportare in output per descrivere le operazioni svolte, presentare i risultati ottenuti e invitare l'utente a usufruire delle matrici di raccomandazione mediante la task apposita. Per quest'ultimo proposito, in questo testo viene anche riportato il token che l'utente deve utilizzare per accedere alle matrici, che sono state salvate su Google Cloud Storage.
L'ultima operazione di language processing riguarda la selezione dell'oggetto e del body per la mail che viene inviata all'utente allegando il report delle vendite in formato PDF ed HTML.
Anche in questi ultimi due casi, per semplicità è stato deciso di non utilizzare dei file di lingua, bensì sono stati semplicemente hardcodati i testi direttamente nel codice.


\section{Report}

Il report delle vendite è il risultato finale della task di analisi delle vendite, e viene generato in formato PDF ed HTML. Il report contiene le statistiche ed i grafici che rappresentano i trend delle vendite nel tempo, e un resoconto finale generato dall'LLM Anthropic.
In questa sezione, vengono descritti i passaggi principali che hanno portato alla generazione del report.

\subsection{PDF}

Il report in formato PDF, in quanto requisito obbligatorio del progetto, è stato sviluppato per primo. Per la sua generazione, sono state prese in considerazione le seguenti librerie:
\begin{itemize}
    \item \textbf{ReportLab}: è una libreria Python per la generazione di documenti PDF, che consente di creare documenti complessi e personalizzati. È stata presa in considerazione per la sua capacità di creare report strutturati con layout avanzati, supporto per grafici e immagini, e controllo dettagliato della formattazione;
    \item \textbf{FPDF}: è una libreria Python per la generazione di documenti PDF, che consente di creare documenti semplici e veloci. È stata presa in considerazione per la sua semplicità di utilizzo e leggerezza, adatta per la generazione rapida di documenti con layout basilari;
    \item \textbf{PyPDF2}: è una libreria Python per la manipolazione di documenti PDF, che consente di unire, dividere e modificare documenti PDF esistenti. È stata presa in considerazione per la possibilità di combinare più sezioni del report o integrare template PDF preesistenti;
    \item \textbf{PDFRW}: è una libreria Python per la manipolazione di documenti PDF, che consente di leggere e scrivere documenti PDF esistenti. È stata presa in considerazione come alternativa a PyPDF2 per operazioni di manipolazione e modifica di documenti PDF già formattati.
\end{itemize}

Alla fine, è stata scelta la libreria ReportLab per la generazione del report in formato PDF, poiché consente di gestire le pagine in modo dinamico e semplice: infatti, all'inserimento di un nuovo flowable (un elemento grafico), ReportLab si occupa automaticamente di creare una nuova pagina se necessario, e di gestire il layout del documento in modo automatico, a differenza di altre librerie che richiedono una gestione manuale delle pagine. Inoltre, ReportLab consente di inserire immagini e grafici in modo semplice e veloce, e di gestire la formattazione del testo in modo avanzato.

Per gestire l'inserimento nel report del resoconto generato dall'LLM, è stato necessario creare una funzione che consentisse di convertire il testo in formato Markdown in flowables di ReportLab. Questa conversione è stata realizzata manualmente, poichè gli output dell'LLM come resoconto sono molto semplici e non contengono elementi complessi come tabelle o liste, e quindi non era necessario utilizzare una libreria esterna per la conversione.


\subsection{HTML}

Dopo aver sviluppato il report in formato PDF ed aver risolto un buon numero di requisiti obbligatori, è stato deciso di sviluppare anche il report in formato HTML, in modo da poterlo visualizzare direttamente nel browser ed avere la possibilità, in futuro, di integrarlo in una pagina web. Per la sua generazione, sono state prese in considerazione le seguenti librerie:
\begin{itemize}
    \item \textbf{Jinja2}: è un motore di template per Python, che consente di generare documenti HTML in modo dinamico e personalizzato. È stata presa in considerazione per la sua capacità di creare template HTML strutturati e riutilizzabili, con supporto per variabili, cicli e condizioni;
    \item \textbf{Mako}: è un altro motore di template per Python, simile a Jinja2, che consente di generare documenti HTML in modo dinamico e personalizzato. È stata presa in considerazione per la sua sintassi semplice e leggibile, e per le sue funzionalità avanzate di templating;
    \item \textbf{Dominate}: è una libreria Python per la generazione di documenti HTML, che consente di creare documenti HTML in modo semplice e veloce, utilizzando una sintassi basata su Python. È stata presa in considerazione per la sua facilità d'uso e per la sua capacità di generare documenti HTML ben formattati;
    \item \textbf{Yattag}: è una libreria Python che permette di generare documenti XML/HTML attraverso un approccio programmatico con metodi Python dedicati. È stata presa in considerazione per la sua sintassi intuitiva che evita la necessità di scrivere tag HTML manualmente e per il suo controllo preciso sulla struttura del documento.
\end{itemize}

Prima di tutto è stato necessario scegliere l'approccio con cui creare l'HTML, tra motore di template e generazione programmatica. È stato allora scelto di utilizzare un motore di template, poiché consente di separare la logica di generazione del report dalla struttura del documento, rendendo il codice più leggibile e manutenibile. Tra i motori di template considerati, è stato scelto Jinja2, poiché è il più diffuso e supportato, e consente di creare template HTML strutturati e riutilizzabili.

Per la generazione del report in formato HTML, è stato dunque creato un template Jinja2 contenente la struttura del documento e i segnaposto per le variabili. Il template è stato progettato per essere facilmente personalizzabile, in modo da poter modificare la struttura del report senza dover riscrivere il codice di generazione. Inoltre, il template consente di inserire le immagini dei grafici codificate in base64, in modo da poterle visualizzare direttamente nel report senza doverle salvare separatamente.

Anche in questo caso, per inserire il resoconto generato dall'LLM, è stato necessario creare una funzione che consentisse di convertire il testo in formato Markdown in HTML. Questa conversione è stata altrettanto realizzata manualmente, per il suddetto motivo legato alla semplicità del resoconto.


\section{Invio di email}

Il più opzionale dei requisiti del progetto riguardanti l'output della task di analisi delle vendite è l'invio di una mail all'utente con il report allegato. Dunque, al termine dello sviluppo della task, è stato implementato anche questo requisito, che consente non solo un'altra modalità di recupero del report, ma anche un'altra modalità di recupero del token per accedere alle matrici di raccomandazione, il quale viene riportato nel corpo della mail.

Di fronte a questo requisito, sono state prese in considerazione varie possibilità.
Inizialmente, si era pensato di inviare la mail da un indirizzo di posta elettronica esistente, ma, dopo consultazione con il tutor, per evitare la condivisione di credenziali riservate, si è deciso invece di contattare tramite API un server SMTP esterno che permetta di inviare mail inserendo un indirizzo di posta elettronica di mittente a piacere. Questo servizio non ha bisogno di un account di posta elettronica esistente, dunque come mittente è stato scelto l'indirizzo \texttt{no-reply@oribea.ai}, il cui nome invita l'utente a non rispondere, considerato che la risposta non verrebbe ricevuta.

La questione successiva è stata la scelta del server SMTP da utilizzare, di fondamentale importanza in quanto rappresenta un investimento economico per l'azienda.
Tuttavia, dopo una ulteriore consultazione con il tutor, si è deciso di utilizzare un server SMTP interno, sviluppato direttamente dall'azienda Oribea appositamente per il progetto, che consenta di inviare mail senza dover pagare un abbonamento.

Una volta realizzato il server SMTP e testato tramite \gls{curl}, è stato possibile implementare una funzione di contatto che imitasse appunto la stringa di comando cURL, e che inviasse una mail all'utente con i file di report allegati. Questa funzione è stata realizzata utilizzando la libreria \gls{requests}, ed è stata inserita nel modulo \emph{report/send\_mail}.

Un problema che si è presentato durante il testing del contatto via codice è stata la codifica dei caratteri speciali (come ad esempio le lettere accentate) nell'oggetto e nel corpo delle mail, che non veniva gestita correttamente dal server SMTP interno. Per risolvere è stato tuttavia sufficiente segnalare il problema al tutor, che ha risolto modificando appositamente il server, garantendo così il supporto all'invio di mail in lingue diverse dall'inglese e contenenti caratteri speciali.

Attualmente il server SMTP interno di Oribea è hostato su una \gls{macchinadev}, e quindi servizi come Gmail e Outlook considerano le mail inviate come spam, e non le recapitano all'utente. Tuttavia, il server SMTP è stato testato con successo e funziona correttamente verso gli altri provider, come ad esempio Libero, e quindi si prevede che in futuro verrà hostato su una macchina di produzione in modo da poter inviare le mail senza problemi di spam.


\section{Le matrici di raccomandazione}

1. Crosstab di Pandas
2. Cosine similarity di Scikit-learn
3. Pandas Vectorized Ops per la norma (no, non citarlo, lo citi solo nell'ottimizzazione)

\subsection{Formato di archiviazione delle matrici}

Considerati l'elevato consumo di spazio e la lentezza di accesso ai dati delle matrici di raccomandazione salvate come file CSV, si è deciso di svolgere un'analisi preliminare per valutare il formato di archiviazione più adatto. Sono stati presi in considerazione i seguenti formati:
\begin{itemize}
    \item \textbf{\gls{csv}}: il formato CSV (Comma-Separated Values) è un formato di testo semplice e ampiamente utilizzato per la memorizzazione di dati tabellari. È stato preso in considerazione per la sua semplicità, portabilità e compatibilità con numerosi strumenti di analisi dati;
    \item \textbf{\gls{hdf5}}: HDF5 (Hierarchical Data Format version 5) è un formato binario progettato per la gestione efficiente di grandi quantità di dati complessi e strutturati. È stato valutato per le sue capacità di compressione, accesso rapido e supporto a dataset multidimensionali;
    \item \textbf{\gls{npy}}: il formato NPY è il formato binario nativo di NumPy per la memorizzazione di array multidimensionali. È stato considerato per la sua efficienza nello storage e nella lettura di array numerici in ambiente Python;
    \item \textbf{\gls{parquet}}: Parquet è un formato di archiviazione colonnare ottimizzato per l'analisi di grandi volumi di dati. È stato preso in considerazione per le sue prestazioni elevate, la compressione e la compatibilità con diversi framework di big data;
    \item \textbf{\gls{zarr}}: Zarr è un formato per la memorizzazione di array multidimensionali che supporta la compressione e l'accesso parallelo ai dati. È stato valutato per la sua scalabilità, flessibilità e facilità di integrazione con sistemi cloud.
\end{itemize}

Sono stati dunque sviluppati dei test automatizzati per confrontare le prestazioni di lettura dei vari formati e lo spazio occupato su disco. I tempi di scrittura non sono stati considerati, poiché le matrici di raccomandazione vengono generate una sola volta e poi salvate su Google Cloud Storage, dove rimangono per essere utilizzate in seguito. I dataset che sono stati utilizzati per i test sono stati scelti cercando di variare la dimensione e la densità delle matrici, e sono qui di seguito elencati in ordine crescente di dimensione della matrice:
\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Dataset} & \textbf{Dimensioni della matrice} \\
        \hline
        Feroze & 10 $\times$ 10 \\
        Vaghasiya & 2.172 $\times$ 268 \\
        Anwer & 795 $\times$ 10.292 \\
        Delikkaya & 35.389 $\times$ 1.000 \\
        Orders\_export & 23.389 $\times$ 4.114 \\
        Cornelius & 14.095 $\times$ 15.000 \\
        \hline
    \end{tabular}
    \caption{Dataset utilizzati per il confronto dei formati di archiviazione delle matrici di raccomandazione}
    \label{tab:dataset-matrici}
\end{table}

I risultati dei test di memoria occupata su disco sono stati i seguenti:

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\columnwidth]{benchmark/memory_usage_by_dataset_and_format.png}
    \caption{Utilizzo della memoria dei formati di archiviazione delle matrici di raccomandazione}
    \label{fig:memory-usage-recommendation-matrices}
\end{figure}

Si evince dunque che i dataset sono classificabili all'incirca così in ordine crescente di spazio occupato su disco:
\begin{enumerate}
    \item Parquet;
    \item Zarr;
    \item NPY;
    \item HDF5;
    \item CSV.
\end{enumerate}

I risultati dei test di velocità di lettura dei vari formati sono stati i seguenti:
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\columnwidth]{benchmark/read_time_by_dataset_and_format.png}
    \caption{Velocità di lettura dei formati di archiviazione delle matrici di raccomandazione}
    \label{fig:read-speed-recommendation-matrices}
\end{figure}

Si evince dunque che i dataset sono classificabili all'incirca così in ordine crescente di velocità di lettura:
\begin{enumerate}
    \item NPY;
    \item Zarr;
    \item HDF5;
    \item Parquet;
    \item CSV.
\end{enumerate}

Il grafico finale che mette assieme i risultati dei test di memoria occupata su disco e velocità di lettura è il seguente:

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\columnwidth]{benchmark/time_vs_memory.png}
    \caption{Utilizzo della memoria e velocità di lettura dei formati di archiviazione delle matrici di raccomandazione}
    \label{fig:memory-usage-and-read-speed-recommendation-matrices}
\end{figure}

Mettendo i valori di memoria usata e velocità di lettura assieme, si evince che il formato Zarr è il migliore per le matrici di raccomandazione, poiché offre un buon compromesso tra spazio occupato su disco e velocità di lettura. Zarr è un formato progettato per la gestione di array multidimensionali e supporta la compressione e l'accesso parallelo ai dati, rendendolo adatto per l'archiviazione delle matrici. Inoltre, Zarr è ottimizzato per l'uso su cloud, il che lo rende compatibile con Google Cloud Storage, dove le matrici di raccomandazione saranno archiviate.

Dunque, si è deciso di utilizzare il formato Zarr per l'archiviazione delle matrici di raccomandazione, poiché offre un buon compromesso tra spazio occupato su disco e velocità di lettura, ed è compatibile con Google Cloud Storage.


\section{La predizione e rank fusion}

\section{Valutazione delle raccomandazioni}

\section{Collegamento con Google Cloud}

Una volta sviluppato il codice delle due tasks funzionante in locale, è giunto il momento di integrarlo nella piattaforma Oribea.
Per fare ciò, ci si è dovuti adattare al funzionamento della piattaforma: infatti, una tipologia di task offre la possibilità di collegarsi ad una Cloud Function ed eseguire il suo codice per costruire l'output da restituire all'utente, che deve essere presentato come file scaricabile. Dunque, dal codice locale si è deciso di ricavare due Cloud Functions, una per l'analisi delle vendite e una per la predizione delle raccomandazioni, che potessero essere collegate a due tasks della piattaforma Oribea e utilizzate dagli utenti della stessa.

\subsection{Google Cloud Functions}

Inizialmente, per imparare come collegarsi a Google Cloud Functions, si è deciso di creare degli esempi il più piccoli possibile, che potessero essere facilmente compresi e modificati, e di partire da un template fornito dalla documentazione aziendale. Si è partiti da un esempio di una Cloud Function che restituisce un file di testo, e sono stati rilevati i primi problemi: la function funzionava da chiamata cURL, ma non da chiamata eseguita dal backend di Oribea, che non riusciva a reperire il file. Dopo aver consultato il team di sviluppatori della piattaforma Oribea, si è compreso che una funzione chiamata \texttt{send\_file}, che era stata rimossa durante la modifica del template, era invece fondamentale per inviare il file generato verso il backend della piattaforma Oribea, che lo avrebbe poi restituito all'utente come file scaricabile. Dunque, si è reinserita la funzione \texttt{send\_file} nel codice della Cloud Function, e da quel momento ha iniziato a funzionare correttamente.

Un lato negativo dell'integrazione tra Oribea e Google Cloud Functions è che la piattaforma Oribea richiede che le Cloud Functions restituiscano un file unico: in cui venga inviato più di un file, viene concesso all'utente di scaricare solo l'ultimo inviato, mentre gli altri vengono ignorati. Dunque, per poter inviare più file, è stato necessario creare un file ZIP che contenesse tutti i file da inviare, e inviare quello ZIP nella chiamata a \texttt{send\_file} che avviene nella Cloud Function. In questo modo, l'utente può scaricare il file ZIP e accedere a tutti i file generati dalla Cloud Function.

Un aspetto da considerare durante il caricamento di una cloud function su Google Cloud è il timeout configurato per la sua creazione. Infatti, la function di Analisi Vendite, avendo un elevato numero di dipendenze da installare, richiede un tempo di caricamento maggiore rispetto al limite predefinito di 540 secondi (9 minuti), in particolare richiede circa 10 minuti. Dunque, si è deciso di aumentare il timeout a 1200 secondi (20 minuti), in modo da garantire che la Cloud Function venga caricata correttamente e che tutte le dipendenze vengano installate senza problemi.

Un altro aspetto importante da considerare durante lo sviluppo delle Cloud Functions è la quantità di memoria RAM disponibile per l'esecuzione del codice. Infatti, le Cloud Functions di Google Cloud hanno un limite di memoria che può essere configurato, e se il codice richiede più memoria di quella disponibile, la funzione non verrà eseguita correttamente. Nel caso delle nostre Cloud Functions, si è deciso di utilizzare 32 GB di RAM invece dei 256 MB predefiniti, poiché il codice richiede una grande quantità di memoria per gestire i dataset e soprattutto la generazione delle matrici di raccomandazione. Utilizzando una quantità maggiore di RAM, si è garantito che le Cloud Functions potessero essere eseguite senza problemi e che le matrici potessero essere generate correttamente.

Inoltre, un altro aspetto importante da considerare durante lo sviluppo delle Cloud Functions è la versione di Python utilizzata. Infatti, le Cloud Functions di Google Cloud supportano diverse versioni di Python, ma alcune librerie potrebbero non essere compatibili con le versioni più vecchie. In particolare, nell'esempio di comando da terminale di configurazione fornito nella documentazione di Oribea, era configurato Python in versione 3.10, e ciò ha causato problemi durante lo sviluppo delle Cloud Functions, poiché dalla libreria Zarr, della quale si è fatto uso per l'archiviazione delle matrici di raccomandazione, sono stati utilizzati dei metodi non funzionanti in un ambiente Python 3.10, e i metodi funzionanti in tale ambiente sono stati dichiarati deprecati al giorno d'oggi, e quindi utilizzarli sarebbe stato una bad practice di ingegneria del software. Dunque, dopo aver compreso il problema, si è deciso di utilizzare Python 3.13, che è l'ultima versione stabile al momento dello sviluppo, e che garantisce la compatibilità con l'ultima versione della libreria Zarr. Utilizzando Python 3.13, si è garantito che le Cloud Functions potessero essere eseguite senza problemi e che le matrici potessero essere archiviate correttamente con dei metodi funzionanti e non deprecati.

Il comando da terminale definito per la creazione delle Cloud Function è stato dunque il seguente:
\begin{verbatim}
gcloud functions deploy function-name ^
    --runtime python313 ^
    --trigger-http ^
    --entry-point processor ^
    --memory 32Gi ^
    --timeout 1200s ^
    --allow-unauthenticated ^
    --region europe-west4
\end{verbatim}

Un'ultima difficoltà incontrata durante lo sviluppo delle Cloud Functions è stata la configurazione del logging. Infatti, la sezione dedicata ai log della piattaforma di Google Cloud Functions, nonostante permetta di visualizzare correttamente i messaggi mostrati con \texttt{print}, ha bisogno di una configurazione particolare per poter visualizzare i log generati dalla libreria \texttt{logging} di Python, altrimenti non verrà mostrato nulla. Per la gestione dei log, è stato creato un logger personalizzato che scrive i messaggi direttamente su \texttt{stdout}, a cui viene successivamente anche aggiunto un handler dedicato alla scrittura su console. 
Ciò non è stato sufficiente: infatti, a questo punto si è visto che i messaggi venivano stampati due volte invece che una. È stato dunque necessario pulire esplicitamente gli altri handlers esistenti e disattivare la propagazione, per evitare duplicati. In questo modo, si è garantito che i messaggi di log potessero essere visualizzati correttamente e una volta sola su console, e che fosse così possibile monitorare l'esecuzione delle Cloud Functions.

\subsection{Google Cloud Storage}

Durante l'attività di analisi dei requisiti del progetto, fin dalla primissima concezione, si è capito che la funzionalità di raccomandazione di prodotti e clienti era da isolare in una task a parte, che potesse essere eseguita in modo indipendente dalla task di analisi delle vendite, che genera le matrici di raccomandazioni. Infatti, le matrici, una volta generate, possono essere utilizzate per generare le raccomandazioni in qualsiasi momento, e ciò è stato visto come un servizio adatto ad una nuova task. Tuttavia, per garantire che la seconda task potesse recuperare le matrici generate dalla prima, era necessario trovare un modo per salvare le matrici in modo persistente. Dunque, siccome le functions sono entrambe caricate ed eseguite in Google Cloud Functions, si è deciso di utilizzare Google Cloud Storage come database accessibile ad entrambe.

Google Cloud Storage consente di creare dei bucket, che sono dei contenitori per i file, assimilabili a delle cartelle, e di gestire i permessi di accesso ai file all'interno di essi. Dunque, si è deciso di creare un nuovo bucket ad ogni esecuzione della task di analisi delle vendite, in modo da poter salvare le matrici di raccomandazione generate in modo persistente e accessibile alla task di raccomandazione. Non solo, si è deciso di salvare nel bucket anche il file csv originale delle vendite e il corrispettivo file preprocessato, per poter accedere ai nomi e agli id di prodotti e clienti, che sono necessari per generare le raccomandazioni.

Per scegliere il nome del bucket, inizialmente si era pensato di utilizzare il nome del file CSV caricato dall'utente, in modo da poter identificare facilmente il bucket associato a quel file. Tuttavia, si è poi capito che questo approccio non era pratico, poiché i nomi dei file potrebbero essere duplicati, anche solo perchè lo stesso utente potrebbe, per errore o per riprovare, eseguire la task due volte inserendo lo stesso file. Si è allora pensato di utilizzare l'indirizzo email dell'utente come nome del bucket, ma anche questo approccio si è rivelato problematico, poiché gli indirizzi email potrebbero contenere caratteri non validi per i nomi dei bucket.
Dunque, per il nome del bucket si è infine deciso di utilizzare un token generato grazie alla libreria \texttt{uuid} di Python, che genera un identificatore unico universale (UUID). Questo token viene poi restituito all'utente mediante inserimento nel \texttt{README} contenuto nel file zip di output e scrittura nella mail che gli viene inviata, in modo che l'utente lo possa copiare per incollarlo nell'apposito campo di input della task di raccomandazione.

Per quanto riguarda l'interazione con Google Cloud Storage via codice, sono state prese in considerazione due librerie:
\begin{itemize}
    \item \textbf{google.cloud.storage}: è la libreria ufficiale di Google per interagire con Google Cloud Storage. Consente di gestire i bucket, caricare e scaricare file, e gestire i permessi di accesso ai file. È stata presa in considerazione per la sua completezza e per il supporto ufficiale di Google;
    \item \textbf{gcsfs}: è una libreria che fornisce un file system virtuale per Google Cloud Storage, consentendo di interagire con i bucket come se fossero normali directory locali. È stata presa in considerazione per la sua facilità d'uso e per la possibilità di utilizzare le funzionalità di Pandas e NumPy per leggere e scrivere file direttamente su Google Cloud Storage.
\end{itemize}

Alla fine, si è deciso di utilizzare la libreria \texttt{google.cloud.storage} nella task di analisi delle vendite e la libreria \texttt{gcsfs} nella task di raccomandazione.
Questo perché la prima libreria consente di gestire i bucket e i file in modo più completo, e quindi è stata reputata più adatta per la creazione di bucket e il caricamento di file, mentre la seconda libreria consente di leggere e scrivere file direttamente su Google Cloud Storage in modo più semplice e veloce, utilizzando le funzionalità di Pandas e NumPy, e quindi è stata reputata più adatta per le semplici operazioni di lettura riservate alla seconda task.
In questo modo, si è garantito che entrambe le task potessero interagire correttamente con Google Cloud Storage e che le matrici di raccomandazione potessero essere salvate e recuperate nel modo più adatto.


\section{Frontend}
\label{sec:frontend}

Dopo aver caricato le due Functions di analisi delle vendite e di raccomandazione su Google Cloud Functions ed averle integrate nella piattaforma Oribea, si è deciso di provare a soddisfare anche il requisito desiderabile che prevede di sviluppare due interfacce grafiche per permettere agli utenti di interagire con le task in modo più semplice e intuitivo. Il reale obiettivo di queste interfacce è quello di garantire una validazione dell'input, assente nella piattaforma Oribea, in modo da evitare errori di contenuto nei file caricati oppure valori errati nei campi di testo, che potrebbero causare errori durante l'esecuzione delle task.

A causa delle difficoltà nello sviluppo di un backend che permetta l'integrazione con Google Cloud Functions, si è deciso di sviluppare due interfacce grafiche scollegate dalla logica funzionale, che possano essere dunque solamente un template personalizzabile che l'azienda Oribea potrà utilizzare per sviluppare le proprie interfacce grafiche in futuro.

Il contenuto effettivo di queste interfacce, data la natura delle tasks, è costituito unicamente da due form di input. Questo requisito ha dunque svolto una parte fondamentale nella scelta delle tecnologie da utilizzare per lo sviluppo.
Un altro requisito è stato quello che le interfacce fossero accessibili tramite un browser web, in modo da poter essere utilizzate da qualsiasi dispositivo connesso a Internet.

I framework di sviluppo web che sono stati presi in considerazione per lo sviluppo delle interfacce grafiche sono stati i seguenti:
\begin{itemize}
    \item \textbf{\gls{angular}}: Angular è un framework web completo sviluppato da Google, basato su TypeScript. Offre una struttura robusta e scalabile per lo sviluppo di applicazioni web complesse, con un'architettura basata su componenti, dependency injection integrato, e un ecosistema ricco di strumenti. È particolarmente adatto per progetti enterprise di grandi dimensioni;
    \item \textbf{\gls{react}}: React è una libreria JavaScript sviluppata da Facebook per la costruzione di interfacce utente interattive. Si basa su un approccio dichiarativo e utilizza un DOM virtuale per ottimizzare le prestazioni. La sua architettura a componenti e la vasta community lo rendono ideale per progetti di varie dimensioni, dalle applicazioni semplici a quelle complesse;
    \item \textbf{\gls{vue}}: Vue è un framework JavaScript progressivo che combina la semplicità di utilizzo con la potenza necessaria per applicazioni complesse. Offre una curva di apprendimento graduale, un sistema di reattività intuitivo e una documentazione eccellente. È particolarmente apprezzato per la sua facilità d'uso e la capacità di integrarsi facilmente in progetti esistenti.
\end{itemize}

Data la grande semplicità delle interfacce grafiche da sviluppare, qualsiasi framework sarebbe stato in grado di soddisfare i requisiti. Allora, la differenza l'ha fatta il consiglio fornito da compagni di corso di laurea che avevano già avuto esperienza con questi framework, che hanno segnalati l'esistenza in React delle seguenti librerie dedicate ai form:
\begin{itemize}
    \item \textbf{\gls{reacthookform}}: è una libreria per la gestione efficiente dei form in React che si basa sull'utilizzo di \gls{hook}. Offre prestazioni elevate minimizzando i re-render dei componenti, validazione integrata, e un'API semplice e intuitiva. È particolarmente apprezzata per la sua leggerezza e per la facilità di integrazione con librerie di validazione esterne;
    \item \textbf{\gls{zod}}: è una libreria di validazione schema-first per TypeScript e JavaScript che consente di definire schemi di validazione type-safe. Permette di validare dati in runtime garantendo al contempo la sicurezza dei tipi in fase di compilazione. È ideale per la validazione di form, API response e input utente;
    \item \textbf{\gls{shadcnui}}: è una collezione di componenti UI riutilizzabili costruiti con Radix UI e Tailwind CSS. Non è propriamente una libreria da installare, ma piuttosto un set di componenti che possono essere copiati e personalizzati direttamente nel progetto. Offre componenti moderni, accessibili e completamente customizzabili che seguono le migliori pratiche di design. In particolare, sono stati consigliati i componenti legati alla creazione di form, come i campi di input, i pulsanti e le etichette.
\end{itemize}

Dunque, grazie a questi consigli e alla semplicità delle interfacce grafiche da sviluppare, si è deciso di utilizzare React come framework di sviluppo, e di usufruire delle librerie React Hook Form, Zod e Shadcn/ui per la gestione dei form e la validazione degli input.

Successivamente, durante la progettazione delle interfacce, si è scelto, di comune accordo con il tutor, di adottare i seguenti approcci:
\begin{itemize}
    \item \textbf{Interfaccia di analisi delle vendite}: l'analisi del caso d'uso del servizio ha portato a progettare un'interfaccia ottimizzata per la visualizzazione a tutto schermo. Infatti, è stato pensato il servizio come standalone, e non come parte di un'applicazione più grande: un tecnico aziendale che deve analizzare le vendite archiviate in un dataset può così accedere al portale e concentrarsi esclusivamente sul fornire l'input al servizio;
    \item \textbf{Interfaccia di raccomandazione}: l'analisi del caso d'uso del servizio ha portato a progettare un'interfaccia ottimizzata per la visualizzazione su una finestrella di dimensioni ridotte. Infatti, è stato pensato il servizio come parte integrata di un'applicazione di e-commerce, che possa essere usata da un utente che sta navigando tra i prodotti e vuole ricevere delle raccomandazioni, oppure da un tecnico aziendale che, navigando tra gli stessi prodotti, vuole prevedere a quali utenti potrebbero interessare. In questo caso, l'interfaccia deve essere semplice, piccola e veloce da utilizzare, senza troppi fronzoli, in modo da non distrarre l'utente dalla navigazione.
\end{itemize}

Un altro aspetto affrontato durante lo sviluppo dell'interfaccia di analisi vendite è stata la scelta di integrare un'anteprima del file csv appena caricato, che ne mostrasse i nomi delle colonne e le prime righe, in modo da permettere all'utente di verificare che il file sia stato caricato correttamente e che contenga i dati attesi. Questa funzionalità è stata implementata utilizzando la libreria React Hook Form per la gestione del form di caricamento del file, e la libreria Papaparse per il parsing del file CSV e la visualizzazione dei dati in anteprima.

Un'anteprima del risultato finale delle interfacce grafiche sviluppate è mostrata nelle figure \ref{fig:frontend-sales-analysis} e \ref{fig:frontend-recommendation}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\columnwidth]{frontend/Frontend Sales Analysis.png}
    \caption{Interfaccia di analisi delle vendite}
    \label{fig:frontend-sales-analysis}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.3\columnwidth]{frontend/Frontend Recommendation.png}
    \caption{Interfaccia di raccomandazione}
    \label{fig:frontend-recommendation}
\end{figure}


\newpage

\subsection{Gestione dinamica del submit}

I submit dei due form delle interfacce grafiche sono stati gestiti in modo dinamico, in modo da poter gestire l'invio dei dati in modo diverso a seconda delle variabili d'ambiente configurate.

Inizialmente, prima ancora della scelta del framework di sviluppo, si era pensato di creare un'interfacce o classe astratta con la quale gestire il submit dei form, in modo da poter implementare più classi concrete che gestissero il submit in modo diverso a seconda dell'ambiente configurato. Tuttavia, dopo aver scelto React come framework di sviluppo, e considerato che React non possiede il concetto di classe, si è deciso di implementare il submit utilizzando gli hook: essi sono funzioni speciali di React che permettono di utilizzare lo stato e altre features all'interno di componenti funzionali, fornendo un'alternativa moderna e più flessibile rispetto ai componenti basati su classi.
La selezione dell'hook corretto da utilizzare è stata fatta tramite uno switch sul valore della variabile d'ambiente \texttt{VITE\_SUBMIT\_STRATEGY}, che può assumere i seguenti valori:
\begin{itemize}
    \item \texttt{api}: utilizza un hook personalizzato per inviare i dati del form tramite chiamata API;
    \item \texttt{log}: utilizza un hook che stampa i risultati del form su console.log;
    \item \texttt{alert}: utilizza un hook che mostra i risultati del form tramite un alert popup.
\end{itemize}

A supporto di questa configurazione, sono stati creati tre file di variabili d'ambiente nella root di entrambi i progetti:
\begin{itemize}
    \item \texttt{.env.development}: contiene le variabili d'ambiente dedicate allo sviluppo e al testing, e la strategia di submit è impostata su \texttt{alert};
    \item \texttt{.env.production}: contiene le variabili d'ambiente dedicate all'ambiente di produzione e all'integrazione con Google Cloud Functions, e la strategia di submit è impostata su \texttt{api};
    \item \texttt{.env.local}: contiene le variabili d'ambiente dedicate all'ambiente locale. Per quanto riguarda i due progetti sviluppati in questo stage, l'ambiente locale e l'ambiente di test sono stati equivalenti, ma è comuque una buona pratica separare un file di variabili d'ambiente locali per eventuali differenziazioni future. A causa dell'attuale equivalenza tra ambiente locale e ambiente di test, questo file contiene le stesse variabili del file \texttt{.env.development}, e quindi la strategia di submit è impostata su \texttt{alert}.
\end{itemize}

Dunque, a seconda della variabile d'ambiente \texttt{VITE\_SUBMIT\_STRATEGY} configurata, il submit dei form delle interfacce grafiche invierà i dati in modo diverso, permettendo di testare i form, in particolare la validazione sviluppata con \texttt{Zod}, senza dover necessariamente inviare i dati a Google Cloud Functions o ad un server esterno.

Attualmente, il submit non è ancora stato integrato con Google Cloud Functions, poiché si è deciso di sviluppare le interfacce grafiche come template personalizzabili, e quindi non sono state implementate le chiamate API per l'invio dei dati. Tuttavia, la struttura del codice è già pronta per essere integrata con le API di Google Cloud Functions in futuro, semplicemente modificando la variabile d'ambiente \texttt{VITE\_SUBMIT\_STRATEGY} in "\texttt{api}" e aggiornando l'apposito \texttt{hook}.



\section{Ottimizzazione}

L'ultima attività svolta durante lo sviluppo della task di analisi delle vendite è stata l'ottimizzazione del codice, in modo da migliorare le prestazioni e ridurre i tempi di esecuzione. In particolare, sono state implementate le seguenti migliorie:

\begin{itemize}
    \item \textbf{Introduzione delle \gls{pandasvectorizedops}}: per migliorare le prestazioni delle operazioni sui dataframe, sono state utilizzate le Pandas Vectorized Ops, che consentono di eseguire operazioni su interi array di dati in modo più efficiente rispetto alle operazioni iterabili, come ad esempio un ciclo for. Questo ha portato a un notevole miglioramento delle prestazioni durante il preprocessing dei dati, il calcolo delle statistiche e la generazione dei grafici;
    \item \textbf{Eliminazione delle operazioni pandasdataframeapply}: sono state eliminate le operazioni pandas.DataFrame.apply, che sono meno efficienti rispetto alle operazioni vettorializzate, e sono state sostituite appunto con queste ultime. Questo ha portato ad un ulteriore miglioramento delle prestazioni nel calcolo delle statistiche e ad un efficientamento nella formattazione delle date e dei prezzi;
    \item \textbf{Miglioramento dell'algoritmo delle etichette}: l'algoritmo per la disambiguazione dei prodotti con lo stesso nome utilizzando etichette ("(1), (2), ..."), utilizzato per il grafico dei top 5 prodotti per fatturato, è stato migliorato in modo da essere più efficiente e veloce. In particolare, è stata creata una funzione dedicata (\texttt{create\_unique\_product\_names}), che utilizza un approccio a due fasi: prima calcola la frequenza di ciascun nome prodotto utilizzando \texttt{value\_counts()}, poi itera una sola volta sui nomi mantenendo un contatore per i duplicati e aggiungendo il suffisso numerico solo ai nomi che compaiono più volte. Questo approccio evita iterazioni multiple e controlli ridondanti, portando a un notevole miglioramento delle prestazioni durante la generazione del grafico.
\end{itemize}

Dopo aver svolto queste ottimizzazioni, sono stati eseguiti dei test di performance per verificare i miglioramenti ottenuti: è stato cioè misurato il tempo di esecuzione della task di analisi vendite prima e dopo l'ottimizzazione. Per ottenere la versione della cloud function non ancora ottimizzata è stato fatto uso del comando \texttt{git switch}, che ha permesso di tornare all'ultimo commit prima dell'ottimizzazione, e sono stati eseguiti i test di performance su quella versione. Poi, è bastato rieseguire \texttt{git switch} verso il commit più recente per fare i test sulla versione ottimizzata. I test cronometrati sono stati eseguiti su tutti i dieci dataset disponibili.
I risultati dei test sono mostrati nella seguente tabella (\ref{tab:performance-tests}).

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Dataset} & \textbf{Tempo pre-otti} & \textbf{Tempo post-otti} & \textbf{Miglioramento (\%)} \\
        \hline
        Swillm & 01:50 & 01:08 & 38,2\% \\
        Dee & 00:58 & 00:38 & 34,5\% \\
        Anwer & 03:01 & 02:00 & 33,7\% \\
        Orders\_export & 03:40 & 02:48 & 23,6\% \\
        Cornelius & 05:21 & 04:34 & 14,6\% \\
        Vaghasiya & 00:53 & 00:49 & 7,5\% \\
        Segura & 00:50 & 00:47 & 6,0\% \\
        Shaw & 00:55 & 00:53 & 3,6\% \\
        Feroze & 00:40 & 00:39 & 2,5\% \\
        Delikkaya & 02:02 & 02:07 & -4,1\% \\
        \hline
    \end{tabular}
    \caption{Risultati dei test di performance prima e dopo l'ottimizzazione}
    \label{tab:performance-tests}
\end{table}

\newpage

Dall'analisi di questi risultati, si distinguono tre fasce di variazione del tempo di esecuzione:
\begin{itemize}
    \item \textbf{Miglioramento elevato}: i dataset Swillm, Dee e Anwer hanno mostrato un miglioramento significativo, con una riduzione del tempo di esecuzione superiore al 30\%. Questo indica che le ottimizzazioni hanno avuto un impatto notevole su dataset di dimensioni maggiori o con una struttura più complessa;
    \item \textbf{Miglioramento moderato}: i dataset Orders\_export, Cornelius, Vaghasiya, Segura e Shaw hanno mostrato un miglioramento compreso tra il 6\% e il 23\%. Questi risultati suggeriscono che le ottimizzazioni hanno avuto un effetto positivo, ma non così marcato come nei casi precedenti;
    \item \textbf{Miglioramento minimo o negativo}: i dataset Feroze e Delikkaya hanno mostrato un miglioramento minimo (2,5\%) o addirittura un peggioramento (-4,1\%). Questo potrebbe essere dovuto alla natura specifica di questi dataset o alla presenza di operazioni che non sono state significativamente influenzate dalle ottimizzazioni.
\end{itemize}

In generale, le ottimizzazioni hanno portato a un miglioramento significativo delle prestazioni della task di analisi delle vendite, con una riduzione media del tempo di esecuzione superiore al 20\%. Tuttavia, è importante notare che i risultati possono variare a seconda della natura specifica dei dataset e delle operazioni svolte su di essi.

Il requisito di ottimizzazione è stato dunque soddisfatto, e le prestazioni della task sono state notevolmente migliorate, rendendo il sistema più efficiente e veloce nell'elaborazione dei dati delle vendite.
